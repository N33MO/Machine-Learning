{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 3\n",
    "\n",
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from urllib import request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Datasets for this assignment stores at the same directory with this file, into two folders as \"train\" and \"test\". There are two predefined classes, which is \"spam\" and \"ham\", and also, each class has one folder in both \"train\" folder and \"test\" folder. Within classes' folders, there are several text file(.txt) store email content which will be trained and test. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Run algorithm here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [\"spam\", \"ham\"]\n",
    "D = \"./train/\"\n",
    "D_test = \"./test/\"\n",
    "doc1 = \"./test/spam/0073.2003-12-24.GP.spam.txt\"\n",
    "doc2 = \"./test/ham/0003.1999-12-14.farmer.ham.txt\"\n",
    "doc3 = \"./test/spam/0889.2004-04-19.GP.spam.txt\"\n",
    "\n",
    "# voc, prior, condprob = TrainMultinomialNB(C, D)\n",
    "\n",
    "# score = ApplyMultinomialNB(C, voc, prior, condprob, doc2)\n",
    "\n",
    "result, overall = evaluateMultinomialNB(C, D, D_test)\n",
    "# print(result)\n",
    "# print(\"overall accuracy: \" + str(overall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Multinomial Naive Bayes\n",
      "----------------------------------------------------------\n",
      "Words filter:\t\tstop_words\n",
      "Result: \n",
      "  spam: \n",
      "\tpositive:\t113\n",
      "\tnegative:\t17\n",
      "\taccuracy: 86.9231%\n",
      "  ham: \n",
      "\tpositive:\t337\n",
      "\tnegative:\t11\n",
      "\taccuracy: 96.8391%\n",
      "  overall: \n",
      "\taccuracy: 94.1423%\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Words filter:\\t\\tstop_words\")\n",
    "print(\"Result: \")\n",
    "print(\"  spam: \")\n",
    "print(\"\\tpositive:\\t\" + str(result['spam']['positive']) + \"\\n\\tnegative:\\t\" + str(result['spam']['negative']))\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(result['spam']['accuracy']))\n",
    "print(\"  ham: \")\n",
    "print(\"\\tpositive:\\t\" + str(result['ham']['positive']) + \"\\n\\tnegative:\\t\" + str(result['ham']['negative']))\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(result['ham']['accuracy']))\n",
    "print(\"  overall: \")\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(overall))\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Multinomial Naive Bayes\n",
    "#### ref: http://nlp.stanford.edu/IR-book/pdf/13bayes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training the data\n",
    "The function below implement multinomial Naive Bayes algorithm for text classification. <br><br>\n",
    "There are three inputs: <br>\n",
    "<b>C</b>: class tag <br>\n",
    "<b>D</b>: document path <br>\n",
    "<b>stop_word_remove</b>: decide whether remove some insignificant terms like punctuations or stop words, there are three possible values: none(default), punctuations, and stop_words <br><br>\n",
    "And there are three outputs: <br>\n",
    "<b>vocabulary</b>: read every terms in training dataset with its frequency (number of times appear in the text) <br>\n",
    "<b>prior</b>: prior probability calculate the probability of each class (number of documents for each class) <br>\n",
    "<b>condprob</b>: conditional probability calculate the probability of each term's frequency in one class <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMultinomialNB(C, D):\n",
    "    \n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    myFilter = stop_words\n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all documents' full path and make its class as keys\n",
    "    files = {c: [] for c in C}\n",
    "    # r: root, d: directories, f: files\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            for c in C:\n",
    "                if '.'+c in file:\n",
    "                    files[c].append(os.path.join(r, file))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all words from each class\n",
    "    vocabulary = {}\n",
    "    for c in C:                                                                           # for each class\n",
    "        for path in files[c]:                                                             # for each document in the class\n",
    "            file = open(path, 'r', encoding='utf-8', errors='ignore')                     # ignore some decoding issues (especialy in emails)\n",
    "            text = \"\"                                                                     # read into a string: 'text'\n",
    "#             for line in file:\n",
    "#                 text = text + line.strip().lower() + \" \"\n",
    "            text = file.read().lower().replace('\\n', ' ')\n",
    "            file.close()\n",
    "            tokens = word_tokenize(text)\n",
    "            filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]         #\n",
    "\n",
    "            for k in filtered_keys:                                                       # apply to dictionary\n",
    "                if k in vocabulary:\n",
    "                    if c in vocabulary[k]:\n",
    "                        vocabulary[k][c] += 1\n",
    "                    else:\n",
    "                        vocabulary[k][c] = 1\n",
    "                else:\n",
    "                    vocabulary[k] = {c: 1}\n",
    "    # regular vocabulary dict by adding 0 value\n",
    "    for k in vocabulary:\n",
    "        for c in C:\n",
    "            if c not in vocabulary[k]:\n",
    "                vocabulary[k][c] = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # prior probability of each class\n",
    "    prior = {}\n",
    "    totalFiles = 0;\n",
    "    for c in C:                                                                           # calculate total number of documents\n",
    "        totalFiles += len(files[c])\n",
    "    for c in C:\n",
    "        prior[c] = len(files[c]) / totalFiles\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate prabability of each word/term\n",
    "    condprob = copy.deepcopy(vocabulary)\n",
    "    denominator = {}\n",
    "    for c in C:                                                                           # calculate total number of words/terms\n",
    "        denominator[c] = 0\n",
    "        for k in vocabulary:\n",
    "            denominator[c] += vocabulary[k][c] + 1                                        # apply laplace smoothing by add 1 to each count\n",
    "    for c in C:\n",
    "        for k in vocabulary:\n",
    "            condprob[k][c] = (vocabulary[k][c] + 1) / denominator[c]\n",
    "            \n",
    "        \n",
    "    return vocabulary, prior, condprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyMultinomialNB(C, V, prior, condprob, d):\n",
    "    \n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    myFilter = stop_words\n",
    "\n",
    "    score = {c: math.log(prior[c]) for c in C}\n",
    "    \n",
    "    file = open(d, 'r', encoding='utf-8', errors='ignore')\n",
    "    text = \"\"\n",
    "#     for line in file:\n",
    "#         text = text + line.strip().lower() + \" \"\n",
    "    text = file.read().lower().replace('\\n', ' ')\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "    \n",
    "    for c in C:\n",
    "        for k in filtered_keys:\n",
    "            if k in V: \n",
    "                score[c] += math.log(condprob[k][c])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateMultinomialNB(C, D, D_test):\n",
    "    \n",
    "    voc, prior, condprob = TrainMultinomialNB(C, D)\n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all documents' full path and make its class as keys\n",
    "    files = {c: [] for c in C}\n",
    "    # r: root, d: directories, f: files\n",
    "    for r, d, f in os.walk(D_test):\n",
    "        for file in f:\n",
    "            for c in C:\n",
    "                if '.'+c in file:\n",
    "                    files[c].append(os.path.join(r, file))\n",
    "    \n",
    "    result = {c: {'positive': 0, 'negative': 0, 'accuracy': 0} for c in C}\n",
    "    for c in C:\n",
    "        for f in files[c]:\n",
    "            score = ApplyMultinomialNB(C, voc, prior, condprob, f)\n",
    "            if score[c] == max(score.values()):\n",
    "                result[c]['positive'] += 1\n",
    "            else:\n",
    "                result[c]['negative'] += 1\n",
    "\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for c in C:\n",
    "        result[c]['accuracy'] = result[c]['positive'] / (result[c]['positive'] + result[c]['negative'])\n",
    "        pos += result[c]['positive']\n",
    "        neg += result[c]['negative']\n",
    "    \n",
    "    overall = pos / (pos + neg)\n",
    "    \n",
    "    return result, overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. MCAP Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMCAPLogisticRegression(C, D):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    \n",
    "    myFilter = stop_words\n",
    "    \n",
    "    # threshold for gradient ascent\n",
    "    threshold = 50\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "                \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all distinct words from training set\n",
    "    idx = 0\n",
    "    vocabulary = {}\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "        text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower().replace('\\n', ' ')\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        for k in filtered_keys:\n",
    "            if k not in vocabulary:\n",
    "                vocabulary[k] = idx\n",
    "                idx += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now there are:\n",
    "    # len(vocabulary) == idx distinct words  (size of array X)\n",
    "    # len(files) documents                   (number of array X)\n",
    "    # idx+1 weights                          (size of array w)\n",
    "    # so we generate matrix X, class y, and vector w\n",
    "    \n",
    "    w = np.ones(idx+1)\n",
    "    X = np.zeros(shape=(len(files), idx))\n",
    "    y = np.zeros(len(files))\n",
    "    \n",
    "    # read all files and update X and y\n",
    "    idx = 0\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "        text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower().replace('\\n', ' ')\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        # update X\n",
    "        for k in filtered_keys:\n",
    "            X[idx][vocabulary[k]] += 1\n",
    "        # update y only if spam\n",
    "        if '.spam' in path:                            # y = 1 for spam email\n",
    "            y[idx] = 1\n",
    "        idx += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now we get w, X, and y\n",
    "    # implement a function for calculate P from w and X[i]\n",
    "    # set η and λ\n",
    "    η = 0.007\n",
    "    λ = 0.005\n",
    "    \n",
    "    # when n > 36, exp(36) / (1 + exp(36)) = 1.0\n",
    "    \n",
    "    w_prev = w\n",
    "    trend = copy.deepcopy(w)\n",
    "    for i in range(threshold):\n",
    "        # ease the final function\n",
    "        exp = w[0] + np.dot(X, w[1:])\n",
    "        exp = np.clip(exp,-36,36)\n",
    "        numerator = np.exp( exp )                                   # for y predict\n",
    "        denominator = 1 + numerator                                 #\n",
    "        y_pred = np.true_divide(numerator, denominator)          # y predict\n",
    "        y_diff = y - y_pred                                         # y diff\n",
    "        func = np.transpose(np.transpose(X) * y_diff)               # sum function\n",
    "        func = func.sum(axis=0)\n",
    "        \n",
    "#         w[0] = w[0] - η * λ * w[0]\n",
    "        w[1:] = w[1:] + η * func - η * λ * w[1:]                    # final function\n",
    "#         if sum(abs(w_prev[1:] - w[1:])) < 1e-6:\n",
    "#             break\n",
    "#         w_prev = w\n",
    "        trend = np.vstack((trend,copy.deepcopy(w)))\n",
    "#         w_prev = w\n",
    "        \n",
    "    \n",
    "    return vocabulary, w, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyMCAPLogisticRegression(C, V, w, d):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    myFilter = stop_words\n",
    "    \n",
    "    \n",
    "    x = np.zeros(len(w)-1)\n",
    "    y = 1 if '.spam' in d else 0\n",
    "    \n",
    "    \n",
    "    file = open(d, 'r', encoding='utf-8', errors='ignore')\n",
    "    text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "    text = file.read().lower().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "    for k in filtered_keys:\n",
    "        if k in V:\n",
    "            x[V[k]] += 1\n",
    "    \n",
    "    exp = w[0] + np.dot(w[1:], x)\n",
    "    exp = np.clip(exp,-36,36)\n",
    "    numerator = np.exp(exp)\n",
    "    denominator = 1 + numerator\n",
    "    y_pred = numerator / denominator\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMCAPLogisticRegression(C, D, D_test):\n",
    "    \n",
    "    voc, w, trend= TrainMCAPLogisticRegression(C, D)\n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    \n",
    "    result = {c: {'positive': 0, 'negative': 0, 'accuracy': 0} for c in C}\n",
    "    \n",
    "    for file in files:\n",
    "        y_pred = ApplyMCAPLogisticRegression(C, voc, w, file)\n",
    "        y = 1 if '.spam' in file else 0\n",
    "        if y == 1:\n",
    "            if y_pred > 0.5:\n",
    "                result['spam']['positive'] += 1\n",
    "            else:\n",
    "                result['spam']['negative'] += 1\n",
    "        else:\n",
    "            if y_pred < 0.5:\n",
    "                result['ham']['positive'] += 1\n",
    "            else:\n",
    "                result['ham']['negative'] += 1\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for c in C:\n",
    "        result[c]['accuracy'] = result[c]['positive'] / (result[c]['positive'] + result[c]['negative'])\n",
    "        pos += result[c]['positive']\n",
    "        neg += result[c]['negative']\n",
    "    \n",
    "    overall = pos / (pos + neg)\n",
    "    \n",
    "    \n",
    "    return result, overall;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-1a91dc38c8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTrainMCAPLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# y1 = ApplyMCAPLogisticRegression(C, voc, w, doc1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-eef7c1abe501>\u001b[0m in \u001b[0;36mTrainMCAPLogisticRegression\u001b[0;34m(C, D)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m)\u001b[0m                                   \u001b[0;31m# for y predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumerator\u001b[0m                                 \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrue_divide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# y predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0my_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m                                         \u001b[0;31m# y diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_diff\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# sum function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "C = [\"spam\", \"ham\"]\n",
    "D = \"./train/\"\n",
    "D_test = \"./test/\"\n",
    "doc1 = \"./test/spam/0073.2003-12-24.GP.spam.txt\"\n",
    "doc2 = \"./test/ham/0003.1999-12-14.farmer.ham.txt\"\n",
    "doc3 = \"./test/spam/0889.2004-04-19.GP.spam.txt\"\n",
    "doc4 = \"./test/ham/0020.1999-12-15.farmer.ham.txt\"\n",
    "\n",
    "\n",
    "voc, w, trend= TrainMCAPLogisticRegression(C, D)\n",
    "\n",
    "# y1 = ApplyMCAPLogisticRegression(C, voc, w, doc1)\n",
    "# y2 = ApplyMCAPLogisticRegression(C, voc, w, doc2)\n",
    "# y3 = ApplyMCAPLogisticRegression(C, voc, w, doc3)\n",
    "# y4 = ApplyMCAPLogisticRegression(C, voc, w, doc4)\n",
    "\n",
    "result, overall = EvaluateMCAPLogisticRegression(C, D, D_test)\n",
    "\n",
    "# print(trend[:, 1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "MCAP Logistic Regression\n",
      "----------------------------------------------------------\n",
      "No. of iterations:\t50\n",
      "Words filter:\t\tstop_words\n",
      "Result: \n",
      "  spam: \n",
      "\tpositive:\t113\n",
      "\tnegative:\t10\n",
      "\taccuracy: 91.8699%\n",
      "  ham: \n",
      "\tpositive:\t334\n",
      "\tnegative:\t6\n",
      "\taccuracy: 98.2353%\n",
      "  overall: \n",
      "\taccuracy: 96.5443%\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------------------------------\")\n",
    "print(\"MCAP Logistic Regression\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"No. of iterations:\\t50\")\n",
    "print(\"Words filter:\\t\\tstop_words\")\n",
    "print(\"Result: \")\n",
    "print(\"  spam: \")\n",
    "print(\"\\tpositive:\\t\" + str(result['spam']['positive']) + \"\\n\\tnegative:\\t\" + str(result['spam']['negative']))\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(result['spam']['accuracy']))\n",
    "print(\"  ham: \")\n",
    "print(\"\\tpositive:\\t\" + str(result['ham']['positive']) + \"\\n\\tnegative:\\t\" + str(result['ham']['negative']))\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(result['ham']['accuracy']))\n",
    "print(\"  overall: \")\n",
    "print(\"\\taccuracy: \" + \"{:.4%}\".format(overall))\n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./test/spam/0073.2003-12-24.GP.spam.txt\"\n",
    "file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "text = \"\"\n",
    "for line in file:\n",
    "    text = text + line.strip().lower() + \" \"\n",
    "file.close()\n",
    "tokens = word_tokenize(text)\n",
    "len(tokens)\n",
    "# tokens[111:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./test/spam/0073.2003-12-24.GP.spam.txt\"\n",
    "file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "text = \"\"\n",
    "# for line in file:\n",
    "#     text = text + line.strip().lower() + \" \"\n",
    "text = file.read().lower().replace('\\n', ' ')\n",
    "file.close()\n",
    "tokens = word_tokenize(text)\n",
    "len(tokens)\n",
    "# tokens[111:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainMultinomialNB(C, D)\n",
    "# V <- ExtractVocabularty(D)\n",
    "# N <- CountDoct(D)\n",
    "# for each c in C\n",
    "# do  N_c <- CountDocsInClass(D, c)\n",
    "#     prior[c] <- N_c/N\n",
    "#     text_c <- ConcatenateTextOfAllDocsInClass(D, c)\n",
    "#     for each t in V\n",
    "#     do  T_ct <- CountTokensOfTerm(text_c, t)\n",
    "#     for each t in V\n",
    "#     do  condprob[t][c] <- (T_ct + 1) / SUM_t^(T_ct^ + 1)\n",
    "# return V, prior, condprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     vocabulary = {c: {} for c in C}\n",
    "#     for c in C:                                                                         # for each class\n",
    "#         for path in files[c]:                                                           # for each document in the class\n",
    "#             file = open(path, 'r', encoding=\"utf-8\", errors=\"ignore\")                   # ignore some decoding issues (especialy for emails)\n",
    "#             text = \"\"                                                                   # read into a string 'text'\n",
    "#             for line in file:\n",
    "#                 text = text + line.strip().lower() + \" \"\n",
    "#             tokens = word_tokenize(text)\n",
    "#             filtered_keys = [i for i in word_tokenize(text) if i not in punctuations]   # remove only punctuations\n",
    "#             for k in filtered_keys:                                                     # apply to dictionary\n",
    "#                 if k in vocabulary[c]:\n",
    "#                     vocabulary[c][k] += 1\n",
    "#                 else:\n",
    "#                     vocabulary[c][k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
