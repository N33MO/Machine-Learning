{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMCAPLogisticRegression(C, D, η, λ, it):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    \n",
    "    myFilter = no_remove\n",
    "    \n",
    "    # threshold for gradient ascent\n",
    "    threshold = it\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "                \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all distinct words from training set\n",
    "    idx = 0\n",
    "    vocabulary = {}\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "#         text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower()\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        filtered_keys = [i for i in text.split()]\n",
    "        for k in filtered_keys:\n",
    "            if k not in vocabulary:\n",
    "                vocabulary[k] = idx\n",
    "                idx += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now there are:\n",
    "    # len(vocabulary) == idx distinct words  (size of array X)\n",
    "    # len(files) documents                   (number of array X)\n",
    "    # idx+1 weights                          (size of array w)\n",
    "    # so we generate matrix X, class y, and vector w\n",
    "    \n",
    "    w = np.ones(idx+1)\n",
    "    X = np.zeros(shape=(len(files), idx))\n",
    "    y = np.zeros(len(files))                           # initialize as 0 (ham)\n",
    "    \n",
    "    # read all files and update X and y\n",
    "    idx = 0\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "#         text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower()\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        filtered_keys = [i for i in text.split()]\n",
    "        # update X\n",
    "        for k in filtered_keys:\n",
    "            X[idx][vocabulary[k]] += 1\n",
    "        # update y only if spam\n",
    "        if '.spam' in path:                            # y = 1 for spam email\n",
    "            y[idx] = 1\n",
    "        idx += 1\n",
    "    X = np.hstack((np.ones(len(files)).reshape(len(files), 1), X))    # appen a ones column as index = 0\n",
    "    \n",
    "    # now we get w, X, and y\n",
    "    # implement a function for calculate P from w and X[i]\n",
    "    # set η and λ\n",
    "#     η = 0.007\n",
    "#     λ = 0.005\n",
    "    \n",
    "    # when n > 36, exp(36) / (1 + exp(36)) = 1.0\n",
    "    \n",
    "    w_prev = w\n",
    "    trend = copy.deepcopy(w)\n",
    "    for i in range(threshold):\n",
    "        # ease the final function\n",
    "        exp = np.dot(X, w)\n",
    "        exp = np.clip(exp,-36,36)\n",
    "        numerator = np.exp( exp )                                   # for y predict\n",
    "        denominator = 1 + numerator                                 #\n",
    "        y_pred = np.true_divide(numerator, denominator)             # y predict\n",
    "        y_diff = y - y_pred                                         # y diff\n",
    "#         func = np.transpose(np.transpose(X) * y_diff)               # sum function\n",
    "#         func = func.sum(axis=0)\n",
    "        func = np.transpose(np.dot(np.transpose(X) , y_diff))\n",
    "        \n",
    "#         w[0] = w[0] - η * λ * w[0]\n",
    "        w = w + η * func - η * λ * w                                # final function\n",
    "#         if sum(abs(w_prev[1:] - w[1:])) < 1e-6:\n",
    "#             break\n",
    "#         w_prev = w\n",
    "        trend = np.vstack((trend,copy.deepcopy(w)))\n",
    "        \n",
    "    \n",
    "    return vocabulary, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyMCAPLogisticRegression(C, V, w, d):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    \n",
    "    myFilter = no_remove\n",
    "    \n",
    "    \n",
    "    x = np.zeros(len(w)-1)\n",
    "    y = 1 if '.spam' in d else 0\n",
    "    \n",
    "    \n",
    "    file = open(d, 'r', encoding='utf-8', errors='ignore')\n",
    "    text = \"\"\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "    filtered_keys = [i for i in text.split()]\n",
    "    for k in filtered_keys:\n",
    "        if k in V:\n",
    "            x[V[k]] += 1\n",
    "    \n",
    "    exp = w[0] + np.dot(w[1:], x)\n",
    "    exp = np.clip(exp,-36,36)\n",
    "    numerator = np.exp(exp)\n",
    "    denominator = 1 + numerator\n",
    "    y_pred = numerator / denominator\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it):\n",
    "    \n",
    "    voc, w = TrainMCAPLogisticRegression(C, D, η, λ, it)\n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    \n",
    "    result = {c: {'positive': 0, 'negative': 0, 'accuracy': 0} for c in C}\n",
    "    \n",
    "    for file in files:\n",
    "        y_pred = ApplyMCAPLogisticRegression(C, voc, w, file)\n",
    "        y = 1 if '.spam' in file else 0\n",
    "        if y == 1:\n",
    "            if y_pred > 0.5:\n",
    "                result['spam']['positive'] += 1\n",
    "            else:\n",
    "                result['spam']['negative'] += 1\n",
    "        else:\n",
    "            if y_pred < 0.5:\n",
    "                result['ham']['positive'] += 1\n",
    "            else:\n",
    "                result['ham']['negative'] += 1\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for c in C:\n",
    "        result[c]['accuracy'] = result[c]['positive'] / (result[c]['positive'] + result[c]['negative'])\n",
    "        pos += result[c]['positive']\n",
    "        neg += result[c]['negative']\n",
    "    \n",
    "    overall = pos / (pos + neg)\n",
    "    \n",
    "    \n",
    "    return result, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAll(result, overall, η, λ, it):\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"                 MCAP Logistic Regression                 \")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    # print(\"No. of iterations:\\t50\")\n",
    "    # print(\"Words filter:\\t\\tnone\\n\")\n",
    "    # print(\"η:\\t\\t\\t0.007\\nλ:\\t\\t\\t0.005\\n\")\n",
    "    # print(\"  spam: \")\n",
    "    # print(\"\\tpositive:\\t\" + str(result['spam']['positive']) + \"\\n\\tnegative:\\t\" + str(result['spam']['negative']))\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(result['spam']['accuracy']))\n",
    "    # print(\"  ham: \")\n",
    "    # print(\"\\tpositive:\\t\" + str(result['ham']['positive']) + \"\\n\\tnegative:\\t\" + str(result['ham']['negative']))\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(result['ham']['accuracy']))\n",
    "    # print(\"  overall: \")\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(overall))\n",
    "\n",
    "    # print(\"spam\\t\\t\\t\\tham\\t\\t\\t\\toverall\")\n",
    "    # print(\"pos\\tneg\\taccuracy\\tpos\\tneg\\taccuracy\\taccuracy\")\n",
    "    # print(str(result['spam']['positive']) + \"\\t\" + str(result['spam']['negative']) + \"\\t\" + \"{:.4%}\".format(result['spam']['accuracy']) + \"\\t\" +  \n",
    "    #       str(result['ham']['positive']) + \"\\t\" + str(result['ham']['negative']) + \"\\t\" + \"{:.4%}\".format(result['ham']['accuracy']) + \"\\t\" + \n",
    "    #       \"{:.4%}\".format(overall)\n",
    "    #      )\n",
    "\n",
    "    print(\"  η = \" + str(η) + \", λ = \" + str(λ) + \", iter = \" + str(it) + \", filter = 'none'\")\n",
    "    print(\"Result:\")\n",
    "\n",
    "    print(\"spam:\\t\\t\" + \"{:.4%}\".format(result['spam']['accuracy']) + \"\\t\" + \"( pos: \" + str(result['spam']['positive']) + \"\\tneg: \" + str(result['spam']['negative']) + \" )\")\n",
    "\n",
    "    print(\"ham:\\t\\t\" + \"{:.4%}\".format(result['ham']['accuracy']) + \"\\t\" + \"( pos: \" + str(result['ham']['positive']) + \"\\tneg: \" + str(result['ham']['negative']) + \" )\")\n",
    "\n",
    "    print(\"overall:\\t\" + \"{:.4%}\".format(overall))\n",
    "\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subject:'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-53680a33b50f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluateMCAPLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mη\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# for it in np.arange(66, 74, 1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-71b850d19b70>\u001b[0m in \u001b[0;36mEvaluateMCAPLogisticRegression\u001b[0;34m(C, D, D_test, η, λ, it)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mEvaluateMCAPLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mη\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainMCAPLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mη\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mλ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# create a dictionary to read all documents' full path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-92-706e291b24fc>\u001b[0m in \u001b[0;36mTrainMCAPLogisticRegression\u001b[0;34m(C, D, η, λ, it)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# update X\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# update y only if spam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'.spam'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m                            \u001b[0;31m# y = 1 for spam email\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject:'"
     ]
    }
   ],
   "source": [
    "C = [\"spam\", \"ham\"]\n",
    "D = \"./train/\"\n",
    "D_test = \"./test/\"\n",
    "\n",
    "η = 0.1\n",
    "λ = 0.01\n",
    "it = 50\n",
    "\n",
    "# voc, w = TrainMCAPLogisticRegression(C, D, η, λ, it)\n",
    "\n",
    "\n",
    "result, overall = EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it)\n",
    "\n",
    "# for it in np.arange(66, 74, 1):\n",
    "#     result, overall = EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it)\n",
    "#     printAll(result, overall, η, λ, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
