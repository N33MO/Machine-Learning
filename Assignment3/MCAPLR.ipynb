{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMCAPLogisticRegression(C, D, η, λ, it):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    \n",
    "    myFilter = no_remove\n",
    "    \n",
    "    # threshold for gradient ascent\n",
    "    threshold = it\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "                \n",
    "    \n",
    "    \n",
    "    # create a dictionary to read all distinct words from training set\n",
    "    idx = 0\n",
    "    vocabulary = {}\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "#         text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower()\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        filtered_keys = [i for i in text.split()]\n",
    "        for k in filtered_keys:\n",
    "            if k not in vocabulary:\n",
    "                vocabulary[k] = idx\n",
    "                idx += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # now there are:\n",
    "    # len(vocabulary) == idx distinct words  (size of array X)\n",
    "    # len(files) documents                   (number of array X)\n",
    "    # idx+1 weights                          (size of array w)\n",
    "    # so we generate matrix X, class y, and vector w\n",
    "    \n",
    "    w = np.ones(idx+1)\n",
    "    X = np.zeros(shape=(len(files), idx))\n",
    "    y = np.zeros(len(files))                           # initialize as 0 (ham)\n",
    "    \n",
    "    # read all files and update X and y\n",
    "    idx = 0\n",
    "    for path in files:\n",
    "        file = open(path, 'r', encoding='utf-8', errors='ignore')\n",
    "#         text = \"\"\n",
    "#         for line in file:\n",
    "#             text = text + line.strip().lower() + \" \"\n",
    "        text = file.read().lower()\n",
    "        file.close()\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "        filtered_keys = [i for i in text.split()]\n",
    "        # update X\n",
    "        for k in filtered_keys:\n",
    "            X[idx][vocabulary[k]] += 1\n",
    "        # update y only if spam\n",
    "        if '.spam' in path:                            # y = 1 for spam email\n",
    "            y[idx] = 1\n",
    "        idx += 1\n",
    "    X = np.hstack((np.ones(len(files)).reshape(len(files), 1), X))    # appen a ones column as index = 0\n",
    "    \n",
    "    # now we get w, X, and y\n",
    "    # implement a function for calculate P from w and X[i]\n",
    "    # set η and λ\n",
    "#     η = 0.007\n",
    "#     λ = 0.005\n",
    "    \n",
    "    # when n > 36, exp(36) / (1 + exp(36)) = 1.0\n",
    "    \n",
    "    w_prev = w\n",
    "    trend = copy.deepcopy(w)\n",
    "    for i in range(threshold):\n",
    "        # ease the final function\n",
    "        exp = np.dot(X, w)\n",
    "        exp = np.clip(exp,-36,36)\n",
    "        numerator = np.exp( exp )                                   # for y predict\n",
    "        denominator = 1 + numerator                                 #\n",
    "        y_pred = np.true_divide(numerator, denominator)             # y predict\n",
    "        y_diff = y - y_pred                                         # y diff\n",
    "#         func = np.transpose(np.transpose(X) * y_diff)               # sum function\n",
    "#         func = func.sum(axis=0)\n",
    "        func = np.transpose(np.dot(np.transpose(X) , y_diff))\n",
    "        \n",
    "#         w[0] = w[0] - η * λ * w[0]\n",
    "        w = w + η * func - η * λ * w                                # final function\n",
    "#         if sum(abs(w_prev[1:] - w[1:])) < 1e-6:\n",
    "#             break\n",
    "#         w_prev = w\n",
    "        trend = np.vstack((trend,copy.deepcopy(w)))\n",
    "        \n",
    "    \n",
    "    return vocabulary, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyMCAPLogisticRegression(C, V, w, d):\n",
    "    \n",
    "    # params for filter out stop words\n",
    "    no_remove = \"\"\n",
    "    punctuations = list(string.punctuation)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words_punc = stopwords.words('english') + list(string.punctuation)\n",
    "    \n",
    "    myFilter = no_remove\n",
    "    \n",
    "    \n",
    "    x = np.zeros(len(w)-1)\n",
    "    y = 1 if '.spam' in d else 0\n",
    "    \n",
    "    \n",
    "    file = open(d, 'r', encoding='utf-8', errors='ignore')\n",
    "    text = \"\"\n",
    "    text = file.read().lower()\n",
    "    file.close()\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_keys = [i for i in word_tokenize(text) if i not in myFilter]\n",
    "    filtered_keys = [i for i in text.split()]\n",
    "    for k in filtered_keys:\n",
    "        if k in V:\n",
    "            x[V[k]] += 1\n",
    "    \n",
    "    exp = w[0] + np.dot(w[1:], x)\n",
    "    exp = np.clip(exp,-36,36)\n",
    "    numerator = np.exp(exp)\n",
    "    denominator = 1 + numerator\n",
    "    y_pred = numerator / denominator\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it):\n",
    "    \n",
    "    voc, w = TrainMCAPLogisticRegression(C, D, η, λ, it)\n",
    "    \n",
    "    # create a dictionary to read all documents' full path\n",
    "    files = []\n",
    "    for r, d, f in os.walk(D):\n",
    "        for file in f:\n",
    "            if '.txt' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    \n",
    "    \n",
    "    result = {c: {'positive': 0, 'negative': 0, 'accuracy': 0} for c in C}\n",
    "    \n",
    "    for file in files:\n",
    "        y_pred = ApplyMCAPLogisticRegression(C, voc, w, file)\n",
    "        y = 1 if '.spam' in file else 0\n",
    "        if y == 1:\n",
    "            if y_pred > 0.5:\n",
    "                result['spam']['positive'] += 1\n",
    "            else:\n",
    "                result['spam']['negative'] += 1\n",
    "        else:\n",
    "            if y_pred < 0.5:\n",
    "                result['ham']['positive'] += 1\n",
    "            else:\n",
    "                result['ham']['negative'] += 1\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for c in C:\n",
    "        result[c]['accuracy'] = result[c]['positive'] / (result[c]['positive'] + result[c]['negative'])\n",
    "        pos += result[c]['positive']\n",
    "        neg += result[c]['negative']\n",
    "    \n",
    "    overall = pos / (pos + neg)\n",
    "    \n",
    "    \n",
    "    return result, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAll(result, overall, η, λ, it):\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(\"                 MCAP Logistic Regression                 \")\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    # print(\"No. of iterations:\\t50\")\n",
    "    # print(\"Words filter:\\t\\tnone\\n\")\n",
    "    # print(\"η:\\t\\t\\t0.007\\nλ:\\t\\t\\t0.005\\n\")\n",
    "    # print(\"  spam: \")\n",
    "    # print(\"\\tpositive:\\t\" + str(result['spam']['positive']) + \"\\n\\tnegative:\\t\" + str(result['spam']['negative']))\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(result['spam']['accuracy']))\n",
    "    # print(\"  ham: \")\n",
    "    # print(\"\\tpositive:\\t\" + str(result['ham']['positive']) + \"\\n\\tnegative:\\t\" + str(result['ham']['negative']))\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(result['ham']['accuracy']))\n",
    "    # print(\"  overall: \")\n",
    "    # print(\"\\taccuracy: \" + \"{:.4%}\".format(overall))\n",
    "\n",
    "    # print(\"spam\\t\\t\\t\\tham\\t\\t\\t\\toverall\")\n",
    "    # print(\"pos\\tneg\\taccuracy\\tpos\\tneg\\taccuracy\\taccuracy\")\n",
    "    # print(str(result['spam']['positive']) + \"\\t\" + str(result['spam']['negative']) + \"\\t\" + \"{:.4%}\".format(result['spam']['accuracy']) + \"\\t\" +  \n",
    "    #       str(result['ham']['positive']) + \"\\t\" + str(result['ham']['negative']) + \"\\t\" + \"{:.4%}\".format(result['ham']['accuracy']) + \"\\t\" + \n",
    "    #       \"{:.4%}\".format(overall)\n",
    "    #      )\n",
    "\n",
    "    print(\"  η = \" + str(η) + \", λ = \" + str(λ) + \", iter = \" + str(it) + \", filter = 'none'\")\n",
    "    print(\"Result:\")\n",
    "\n",
    "    print(\"spam:\\t\\t\" + \"{:.4%}\".format(result['spam']['accuracy']) + \"\\t\" + \"( pos: \" + str(result['spam']['positive']) + \"\\tneg: \" + str(result['spam']['negative']) + \" )\")\n",
    "\n",
    "    print(\"ham:\\t\\t\" + \"{:.4%}\".format(result['ham']['accuracy']) + \"\\t\" + \"( pos: \" + str(result['ham']['positive']) + \"\\tneg: \" + str(result['ham']['negative']) + \" )\")\n",
    "\n",
    "    print(\"overall:\\t\" + \"{:.4%}\".format(overall))\n",
    "\n",
    "    print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [\"spam\", \"ham\"]\n",
    "D = \"./train/\"\n",
    "D_test = \"./test/\"\n",
    "\n",
    "η = 0.1\n",
    "λ = 0.01\n",
    "it = 50\n",
    "\n",
    "voc, w = TrainMCAPLogisticRegression(C, D, η, λ, it)\n",
    "\n",
    "\n",
    "# result, overall = EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it)\n",
    "\n",
    "# for it in np.arange(66, 74, 1):\n",
    "#     result, overall = EvaluateMCAPLogisticRegression(C, D, D_test, η, λ, it)\n",
    "#     printAll(result, overall, η, λ, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
